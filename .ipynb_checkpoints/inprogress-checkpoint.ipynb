{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaser/miniconda3/envs/dantemp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets, model_selection\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import resource_variable_ops as rr\n",
    "import numpy as np; np.seed=5\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = datasets.load_breast_cancer()\n",
    "data = datasets.load_boston()\n",
    "\n",
    "indata = data.data\n",
    "outdata = data.target.reshape(-1,1)\n",
    "\n",
    "in_scaler = StandardScaler()\n",
    "indata = in_scaler.fit_transform(indata)\n",
    "\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = model_selection.train_test_split(indata, outdata, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('dde/inputs_train', inputs_train)\n",
    "np.save('dde/inputs_test', inputs_test)\n",
    "np.save('dde/outputs_train', outputs_train)\n",
    "np.save('dde/outputs_test', outputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs_train = np.load('dde/inputs_train.npy')\n",
    "inputs_test = np.load('dde/inputs_test.npy')\n",
    "outputs_train = np.load('dde/outputs_train.npy')\n",
    "outputs_test = np.load('dde/outputs_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the optimizer\n",
    "\n",
    "\n",
    "class GradientProcessor(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,shape, gamma, window):\n",
    "        self.shape = shape\n",
    "        self.epsilon = 1e-5\n",
    "        self.gamma = gamma\n",
    "        self.window = window\n",
    "        \n",
    "        self.grad_hist1 = rr.ResourceVariable(tf.zeros(shape=shape,dtype=tf.float32))\n",
    "        self.para_hist1 = rr.ResourceVariable(tf.zeros(shape=shape,dtype=tf.float32))\n",
    "        self.grad_hist2 = rr.ResourceVariable(tf.zeros(shape=shape,dtype=tf.float32))\n",
    "        self.para_hist2 = rr.ResourceVariable(tf.zeros(shape=shape,dtype=tf.float32))\n",
    "        self.grad_hist3 = rr.ResourceVariable(tf.zeros(shape=shape,dtype=tf.float32))\n",
    "        self.para_hist3 = rr.ResourceVariable(tf.zeros(shape=shape,dtype=tf.float32))\n",
    "        self.grad_hist4 = rr.ResourceVariable(tf.zeros(shape=shape,dtype=tf.float32))\n",
    "        self.para_hist4 = rr.ResourceVariable(tf.zeros(shape=shape,dtype=tf.float32))\n",
    "        \n",
    "        self.mock_zeros = rr.ResourceVariable(tf.zeros(shape=shape,dtype=tf.float32))\n",
    "        \n",
    "    def _enqueue(self, new_grad, new_para):\n",
    "        \n",
    "        old_grad_hist1 = self.grad_hist1.read_value()\n",
    "        old_para_hist1 = self.para_hist1.read_value()\n",
    "        old_grad_hist2 = self.grad_hist2.read_value()\n",
    "        old_para_hist2 = self.para_hist2.read_value()\n",
    "        old_grad_hist3 = self.grad_hist2.read_value()\n",
    "        old_para_hist3 = self.para_hist2.read_value()\n",
    "        \n",
    "        with tf.control_dependencies([old_grad_hist1, old_grad_hist2, old_para_hist1, old_para_hist2, old_para_hist3, old_para_hist3]):\n",
    "            assign_gh4 = tf.assign(self.grad_hist4, old_grad_hist3)\n",
    "            assign_ph4 = tf.assign(self.para_hist4, old_para_hist3)\n",
    "            assign_gh3 = tf.assign(self.grad_hist3, old_grad_hist2)\n",
    "            assign_ph3 = tf.assign(self.para_hist3, old_para_hist2)\n",
    "            assign_gh2 = tf.assign(self.grad_hist2, old_grad_hist1)\n",
    "            assign_ph2 = tf.assign(self.para_hist2, old_para_hist1)\n",
    "            assign_gh1 = tf.assign(self.grad_hist1, new_grad)\n",
    "            assign_ph1 = tf.assign(self.para_hist1, new_para)\n",
    "            \n",
    "        return assign_gh2, assign_ph2, assign_gh1, assign_ph1\n",
    "        \n",
    "    def _calc_dist(self, final, initial):\n",
    "        return final - initial\n",
    "    \n",
    "    def _calc_norm(self, vector):\n",
    "        return tf.linalg.norm(vector, ord='euclidean')\n",
    "        \n",
    "    def calc_deltagrad(self, fin_grad, ini_grad):\n",
    "        return self._calc_dist(fin_grad, ini_grad)\n",
    "    \n",
    "    def calc_stepsize(self, fin_para, ini_para):\n",
    "        deltapara = self._calc_dist(fin_para, ini_para)\n",
    "        delta_s = self._calc_norm(deltapara) + self.epsilon # must add epsilon for numerical stability\n",
    "        return deltapara, delta_s\n",
    "    \n",
    "    def calc_dirdiv(self, deltagrad, stepsize):\n",
    "        return tf.divide(deltagrad,stepsize)\n",
    "        \n",
    "    def calc_proj(self, cur_grad, deltapara, stepsize):\n",
    "        return tf.tensordot(tf.reshape(cur_grad,[-1]), tf.reshape(deltapara,[-1]),axes=1) / stepsize\n",
    "    \n",
    "    def calc_x(self, proj, dirdiv):\n",
    "        return proj * dirdiv\n",
    "    \n",
    "    def calc_gamma(self, cur_para, fin_para, ini_para):\n",
    "        \"\"\"\n",
    "        This is designed so that both distances have to be small in order for \n",
    "        the second order effect to be significant. If the distance between final and initial\n",
    "        is large then the second order approxmiation will be very bad. If the distance between\n",
    "        current and initial is large than even if the curvature approxmiation at initial is accurate,\n",
    "        it may not apply at the current position (i.e. 3rd and higher order effects).\n",
    "        \n",
    "        NOTE: This did not work due to numerical issues. The distances are so small that this\n",
    "        number blows up the update.\n",
    "        \n",
    "        gamma = 1/|d1| * 1/|d2|\n",
    "            - If cur_para is fin_para then gamma = 1/(d^2).\n",
    "            - I'd ideally avoid using the Euclidean distance so that this is more scale-invariant;\n",
    "            the L2 norm would be dominated by relatively small changes in large value params\n",
    "            (though I'm not sure if this succeeds)\n",
    "        \"\"\"\n",
    "        dist_eval = self._calc_dist(fin_para, ini_para)\n",
    "        dist_cur = self._calc_dist(cur_para, ini_para)\n",
    "        return self._calc_norm(tf.multiply((1 / (1 + tf.abs(dist_eval))),(1 / (1 + tf.abs(dist_cur))))) + self.epsilon\n",
    "    \n",
    "    def calc_combined_x(self, xs):\n",
    "        return tf.reduce_sum(tf.stack(xs))\n",
    "    \n",
    "    def calc_update(self, cur_grad, x, gamma):\n",
    "        return cur_grad + tf.multiply(x, gamma)\n",
    "    \n",
    "    def calc_dist(self, cur_para, ini_para):\n",
    "        return self._calc_norm(self._calc_dist(cur_para, ini_para))\n",
    "    \n",
    "    def subprocess(self, cur_grad, cur_para, fin_grad, fin_para, ini_grad, ini_para):\n",
    "        deltagrad = self.calc_deltagrad(fin_grad, ini_grad)\n",
    "        deltapara, delta_s = self.calc_stepsize(fin_para, ini_para)\n",
    "        dirdiv = self.calc_dirdiv(deltagrad, delta_s)\n",
    "        proj = self.calc_proj(cur_grad, deltapara, delta_s)\n",
    "        x = self.calc_x(proj, dirdiv)\n",
    "        dist = self.calc_dist(cur_para, ini_para)\n",
    "        return x, dist + delta_s\n",
    "    \n",
    "    def process(self, grad, para):\n",
    "            \n",
    "        queued = self._enqueue(grad, para)\n",
    "        with tf.control_dependencies(queued):\n",
    "            x_12, d_12 = self.subprocess(self.grad_hist1.read_value(), self.para_hist1.read_value(), self.grad_hist1.read_value(), self.para_hist1.read_value(), self.grad_hist2.read_value(), self.para_hist2.read_value())\n",
    "            x_13, d_13 = self.subprocess(self.grad_hist1.read_value(), self.para_hist1.read_value(), self.grad_hist1.read_value(), self.para_hist1.read_value(), self.grad_hist3.read_value(), self.para_hist3.read_value())\n",
    "            x_23, d_23 = self.subprocess(self.grad_hist1.read_value(), self.para_hist1.read_value(), self.grad_hist2.read_value(), self.para_hist2.read_value(), self.grad_hist3.read_value(), self.para_hist3.read_value())\n",
    "            x_14, d_14 = self.subprocess(self.grad_hist1.read_value(), self.para_hist1.read_value(), self.grad_hist1.read_value(), self.para_hist1.read_value(), self.grad_hist4.read_value(), self.para_hist4.read_value())\n",
    "            x_34, d_34 = self.subprocess(self.grad_hist1.read_value(), self.para_hist1.read_value(), self.grad_hist3.read_value(), self.para_hist3.read_value(), self.grad_hist4.read_value(), self.para_hist4.read_value())\n",
    "            x_24, d_24 = self.subprocess(self.grad_hist1.read_value(), self.para_hist1.read_value(), self.grad_hist2.read_value(), self.para_hist2.read_value(), self.grad_hist4.read_value(), self.para_hist4.read_value())\n",
    "      \n",
    "        xs = [x_c1]\n",
    "        if self.window > 1:\n",
    "            xs.append(x_c2, x_12)\n",
    "        if self.window > 2:\n",
    "            xs.append(x_c3, x_23, x_13)\n",
    "        with tf.control_dependencies(xs):\n",
    "            combined_x = self.calc_combined_x(xs)\n",
    "        \n",
    "        processed_grad = self.calc_update(grad, combined_x, self.gamma)\n",
    "        return processed_grad\n",
    "\n",
    "        \n",
    "def process_gradients(processors, grads_and_vars):\n",
    "    return [(processor.process(gv[0],gv[1]), gv[1]) for processor, gv in zip(processors,grads_and_vars)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 13\n",
    "n_output = 1\n",
    "n_hidden = 100\n",
    "n_layers = 20\n",
    "\n",
    "ini_mean = 0\n",
    "ini_stddev = 1/(2*n_hidden)\n",
    "\n",
    "window = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network built\n"
     ]
    }
   ],
   "source": [
    "# the network\n",
    "\n",
    "\n",
    "hidden_dict = {}\n",
    "\n",
    "num_params = (n_input*n_hidden) + n_hidden + (n_hidden*n_hidden*(n_layers)) + (n_output*n_hidden) + n_output\n",
    "\n",
    "\n",
    "activation = tf.nn.relu\n",
    "\n",
    "def weight_matrix(n_in,n_out):\n",
    "    return tf.Variable(\n",
    "        tf.truncated_normal(shape=(n_in,n_out), mean=ini_mean, stddev=ini_stddev)\n",
    "        ,dtype=tf.float32)\n",
    "\n",
    "def bias_matrix(n_to):\n",
    "    return tf.Variable(tf.zeros(shape=(1,n_to),dtype=tf.float32))\n",
    "           \n",
    "    \n",
    "\n",
    "input_layer = tf.placeholder(shape=(None,n_input),dtype=tf.float32)\n",
    "output_truth = tf.placeholder(shape=(None,n_output),dtype=tf.float32)\n",
    "gamma = tf.placeholder(shape=(),dtype=tf.float32)\n",
    "\n",
    "\n",
    "for n in range(1,n_layers+1):\n",
    "    hidden_dict[n] = {}\n",
    "    if n == 1:\n",
    "        hidden_dict[n]['weights'] = weight_matrix(n_input,n_hidden)\n",
    "        hidden_dict[n]['bias'] = bias_matrix(n_hidden)\n",
    "        hidden_dict[n]['layer'] = activation(tf.matmul(input_layer,hidden_dict[n]['weights']) + hidden_dict[n]['bias'])\n",
    "    else:\n",
    "        hidden_dict[n]['weights'] = weight_matrix(n_hidden,n_hidden)\n",
    "        hidden_dict[n]['bias'] = bias_matrix(n_hidden)\n",
    "        hidden_dict[n]['layer'] = activation(tf.matmul(hidden_dict[n-1]['layer'],hidden_dict[n]['weights']) + hidden_dict[n]['bias'])\n",
    "        \n",
    "\n",
    "output_weights = weight_matrix(n_hidden,n_output)\n",
    "output_bias = bias_matrix(n_output)\n",
    "\n",
    "output_pre = tf.matmul(hidden_dict[n_layers]['layer'],output_weights) + output_bias\n",
    "#output_pred = tf.nn.sigmoid(output_pre)\n",
    "output_pred = output_pre\n",
    "                 \n",
    "loss = tf.reduce_sum(tf.reduce_mean((1/2)*(output_truth - output_pred)**2))\n",
    "#loss = -tf.reduce_sum(tf.reduce_mean(output_truth*tf.log(output_pred)+(1-output_truth)*tf.log(1-output_pred)))\n",
    "    \n",
    "lr = tf.placeholder(shape=(),dtype=tf.float32)\n",
    "#opt = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "#opt = tf.train.AdamOptimizer(learning_rate=lr,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False)\n",
    "opt = tf.train.MomentumOptimizer(learning_rate=lr,momentum=0.9)\n",
    "\n",
    "all_variables = [hidden_dict[n]['weights'] for n in range(1,n_layers+1)] + [output_weights] + [hidden_dict[n]['bias'] for n in range(1,n_layers+1)] + [output_bias]\n",
    "processors = [GradientProcessor(shape = variable.shape, gamma = gamma, window = window) for variable in all_variables]\n",
    "\n",
    "control_opt = opt.minimize(loss, var_list = all_variables)\n",
    "\n",
    "correct_prediction = tf.equal(output_truth, tf.round(output_pred))\n",
    "#acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "grads_and_vars = opt.compute_gradients(loss, all_variables)\n",
    "\n",
    "processed_gradients = process_gradients(processors, grads_and_vars)\n",
    "with tf.control_dependencies([g[0] for g in processed_gradients]):\n",
    "    custom_opt = opt.apply_gradients(processed_gradients)\n",
    "\n",
    "print('network built')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': <tf.Variable 'Variable_4075:0' shape=(1, 100) dtype=float32_ref>,\n",
       " 'layer': <tf.Tensor 'Relu_260:0' shape=(?, 100) dtype=float32>,\n",
       " 'weights': <tf.Variable 'Variable_4074:0' shape=(13, 100) dtype=float32_ref>}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training(inputs, outputs, n_epochs, learning_rate, _gamma, tqdm_on=False, batch_size=64, control=True):\n",
    "    \n",
    "    inputs_train, inputs_test = inputs\n",
    "    outputs_train, outputs_test = outputs\n",
    "    \n",
    "    if tqdm_on is True: pbar = tqdm(range(n_epochs))\n",
    "    else: pbar = range(n_epochs)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    grads = []\n",
    "    proc_grads = []\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        if control is True:\n",
    "            optimizer = control_opt\n",
    "        else:\n",
    "            optimizer = custom_opt\n",
    "        \n",
    "        for _ in pbar:\n",
    "            \n",
    "            batch_idx = np.random.choice(np.arange(len(inputs_train)), size = batch_size)\n",
    "            input_batch = inputs_train[batch_idx]\n",
    "            output_batch = outputs_train[batch_idx]\n",
    "            \n",
    "            \n",
    "            \n",
    "            _, _proc_grads, _loss, pred, _grads  = sess.run([optimizer, processed_gradients, loss, output_pred, grads_and_vars], \n",
    "                                                  feed_dict={input_layer: input_batch\n",
    "                                                             , output_truth: output_batch\n",
    "                                                             , lr: learning_rate\n",
    "                                                             , gamma: _gamma\n",
    "                                                            })\n",
    "            _test_loss = sess.run(loss, feed_dict={input_layer:inputs_test, output_truth:outputs_test})\n",
    "            \n",
    "            train_losses.append(_loss)\n",
    "            test_losses.append(_test_loss)\n",
    "            grads.append(_grads)\n",
    "            proc_grads.append(_proc_grads)\n",
    "            \n",
    "            if tqdm_on is True:\n",
    "                pbar.set_description('train: {:2f}, test: {:2f}'.format(_loss,_test_loss))\n",
    "\n",
    "    return train_losses, test_losses, grads, proc_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_exps(num_exps, name, tqdm_on=False, epochs=15, lrate=0.1, _gamma=0.5, batch_size=64, control=True):\n",
    "    \n",
    "    df_test = None\n",
    "    times = []\n",
    "\n",
    "    for exp in range(1,num_exps+1):\n",
    "        print('starting exp {}'.format(exp))\n",
    "        st = time.time()\n",
    "        losses_train, losses_test, grads, proc_grads = run_training((inputs_train, inputs_test), (outputs_train, outputs_test), n_epochs=epochs, learning_rate = lrate, _gamma = _gamma, tqdm_on=tqdm_on, batch_size=batch_size, control=control)\n",
    "        elapsed = int(time.time()-st)\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        losses = np.concatenate([np.array(losses_train).reshape(-1,1), np.array(losses_test).reshape(-1,1)],axis=1)\n",
    "        df = pd.DataFrame(losses)\n",
    "        \n",
    "        df.columns = ['loss_train','loss_test']\n",
    "        df['experiment'] = exp\n",
    "        df['depth'] = n_layers\n",
    "        df['width'] = n_hidden\n",
    "        df['batch_size'] = batch_size\n",
    "        df['lr'] = lrate\n",
    "        df['gamma'] = _gamma\n",
    "        df['name'] = name\n",
    "        df['window'] = window\n",
    "\n",
    "        if df_test is None:\n",
    "            df_test = df\n",
    "        else:\n",
    "            df_test = pd.concat([df_test,df],axis=0)\n",
    "            \n",
    "\n",
    "    meantime = int(np.array(times).mean())\n",
    "    df_test.reset_index().rename(columns={'index':'epoch'})\n",
    "    df_test.to_csv('dde/l{}_h{}_bs{}_lr{}_g{}_t{}_m0.9_{}.csv'.format(n_layers, n_hidden, batch_size, lrate, _gamma, meantime, name))\n",
    "            \n",
    "    return df_test, grads, proc_grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#exps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting exp 1\n",
      "starting exp 2\n"
     ]
    }
   ],
   "source": [
    "name = 'g1000.0'\n",
    "\n",
    "exps[name], grads, proc_grads = run_exps(\n",
    "    num_exps = 2,\n",
    "    tqdm_on=False,\n",
    "    epochs=100,\n",
    "    lrate=0.001,\n",
    "    _gamma=1000.0,\n",
    "    batch_size=128,\n",
    "    control=False,\n",
    "    name=name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del exps['g0.1']\n",
    "del exps['g0.01']\n",
    "del exps['g10.0']\n",
    "del exps['g0.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_control = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_list = [df_control, df_control_x3] + [df_xw4, df_xw3, df_xw2, df_xw1, df_xw4_05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['control', 'g1000.0'])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_list = [exps[show] for show in exps.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tot = pd.concat(test_list,axis=0)\n",
    "df_tot = df_tot.reset_index(drop=False).rename(columns={'index':'epoch'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaser/miniconda3/envs/dantemp/lib/python3.6/site-packages/seaborn/timeseries.py:183: UserWarning: The tsplot function is deprecated and will be removed or replaced (in a substantially altered version) in a future release.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCOHm/wEsBsAP9K2KPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48200770f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.tsplot(time=\"epoch\", value=\"loss_test\",\n",
    "                 unit=\"experiment\", condition=\"name\",\n",
    "                 data=df_tot, ci=[68]\n",
    "          ,estimator=np.median)\n",
    "\n",
    "#plt.ylim(0,400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exps['test1']['name'] = 'dde'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_window3['name'] = 'dde1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(grads[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_grads = [layer for layer in [epoch for epoch in grads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(np.stack(all_grads),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "choice = 'param'\n",
    "\n",
    "if choice == 'grad':\n",
    "    c = 0\n",
    "else:\n",
    "    c = 1\n",
    "\n",
    "ps = []\n",
    "gs = []\n",
    "pgs = []\n",
    "l = 3\n",
    "fro = 300\n",
    "to = 300\n",
    "\n",
    "fro_rand = np.random.randint(fro)\n",
    "to_rand = np.random.randint(to)\n",
    "\n",
    "for e, pe in zip(grads,proc_grads):\n",
    "    \n",
    "    gs.append(e[l][0][fro_rand][to_rand])\n",
    "    ps.append(e[l][1][fro_rand][to_rand])\n",
    "    pgs.append(pe[l][0][fro_rand][to_rand])\n",
    "\n",
    "gs = np.stack(gs)\n",
    "ps = np.stack(ps)\n",
    "pgs = np.stack(pgs)\n",
    "\n",
    "print(gs.sum())\n",
    "print(ps.sum())\n",
    "print(pgs.sum())\n",
    "\n",
    "plt.plot(range(len(gs)),gs)\n",
    "plt.title('partial derivative - layer {} weight {},{}'.format(l,fro_rand,to_rand))\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(ps)),ps)\n",
    "plt.title('parameter - layer {} weight {},{}'.format(l,fro_rand,to_rand))\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(pgs)),pgs)\n",
    "plt.title('processed partial derivative - layer {} weight {},{}'.format(l,fro_rand,to_rand))\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sums = []\n",
    "\n",
    "for l in range(len(grads[10])):\n",
    "    \n",
    "    gs = []\n",
    "    \n",
    "    gs.append(np.sum(grads[2][l][0]))\n",
    "    gs = np.stack(gs)\n",
    "    sums.append(gs.sum())\n",
    "\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grads[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grads[10][300][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(grads[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dantemp]",
   "language": "python",
   "name": "conda-env-dantemp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
